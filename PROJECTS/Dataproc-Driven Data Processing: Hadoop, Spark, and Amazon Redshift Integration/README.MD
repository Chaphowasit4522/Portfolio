# Dataproc-Driven Data Processing: Hadoop, Spark, and Amazon Redshift Integration üêò

# Introduction
Embark on a transformative data processing journey with our project, "Dataproc-Driven Data Processing: Hadoop, Spark, and Redshift Integration." This endeavor leverages the prowess of Google Cloud Dataproc to orchestrate a seamless Hadoop-Spark pipeline, offering efficient data manipulation and analytics on Google Cloud Storage. Integrated with Jupyter Notebook and powered by PySpark, our solution ensures user-friendly data exploration. The final destination for our processed data is AWS Redshift, a fully managed data warehouse, promising scalability and accessibility for comprehensive business intelligence and analytics. Join us in navigating the convergence of Dataproc, Hadoop, Spark, and Redshift, where data processing becomes a unified and insightful experience.

# Data Description
The dataset, spanning from 2014 to 2017, encapsulates information on crime incidents with a focus on major offenses. Each record details the geographic coordinates (X, Y) of the event location, a unique event identifier (event_unique_id), occurrence and reported dates, premise type (premisetype), Uniform Crime Reporting (UCR) codes, offense type, and various temporal components such as reported and occurrence years, months, days, and hours. Additional attributes include the Major Crime Indicator (MCI) category, police division (Division), neighborhood ID (Hood_ID), neighborhood name, and geographical coordinates (Lat, Long). This dataset serves as a valuable resource for analyzing crime patterns, trends, and their spatial-temporal dynamics over the specified period.

# Technologies & Tools
Big Data Processing: Apache Hadoop, Apache Spark <br>
Cloud Platform: GCP, AWS <br> 
Data Exploration: Jupyter Notebooks with Python <br>
Data Storage: Google Cloud Storage (GCS), AWS Redshift <br>
Programming Languages: Python (PySpark and Pandas) <br>

